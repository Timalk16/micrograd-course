# Micrograd: Building an Autograd Engine from Scratch

<div align="center">

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![Jupyter](https://img.shields.io/badge/Jupyter-Notebook-orange.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)

**A minimal implementation of automatic differentiation and neural networks from first principles**

[Overview](#overview) â€¢ [Features](#features) â€¢ [Implementation Details](#implementation-details) â€¢ [Usage](#usage) â€¢ [Learning Outcomes](#learning-outcomes)

</div>

---

## ğŸ“š Overview

This project is an educational implementation of **micrograd** - a minimal autograd engine that demonstrates the fundamental principles behind automatic differentiation and neural network training. Built from scratch using only Python and NumPy, this implementation provides deep insights into how modern deep learning frameworks like PyTorch and TensorFlow work under the hood.

### What is Autograd?

Automatic differentiation (autograd) is the technique that powers modern deep learning. Instead of manually computing derivatives, autograd engines automatically track operations and compute gradients using the chain rule, enabling efficient backpropagation through complex computational graphs.

---

## âœ¨ Features

### Core Components

- **`Value` Class**: A scalar value wrapper that tracks computation graphs
  - Automatic gradient computation via backpropagation
  - Support for basic operations: `+`, `-`, `*`, `/`, `**`, `tanh`, `exp`
  - Topological sorting for efficient gradient flow

- **Neural Network Architecture**:
  - `Neuron`: Single neuron with weights, bias, and activation
  - `Layer`: Collection of neurons forming a layer
  - `MLP`: Multi-Layer Perceptron for building deep networks

- **Visualization**: Computation graph visualization using Graphviz

### Key Capabilities

âœ… Forward and backward propagation  
âœ… Gradient computation for all operations  
âœ… Neural network training with gradient descent  
âœ… Computation graph visualization  
âœ… Comparison with PyTorch implementation  

---

## ğŸ—ï¸ Implementation Details

### The Value Class

The heart of this implementation is the `Value` class, which wraps scalar values and tracks:

- **Data**: The actual numerical value
- **Gradient**: The derivative with respect to the output
- **Operation Graph**: Parent nodes and operations performed
- **Backward Function**: Local derivative computation

```python
class Value:
    def __init__(self, data, _children=(), _op='', label=''):
        self.data = data
        self.grad = 0.0
        self._backward = lambda: None
        self._prev = set(_children)
        self._op = _op
        self.label = label
```

### Automatic Differentiation

The engine uses **reverse-mode automatic differentiation** (backpropagation):

1. **Forward Pass**: Build computation graph while performing operations
2. **Backward Pass**: Traverse graph in reverse topological order
3. **Chain Rule**: Multiply local derivatives by upstream gradients

### Example: Simple Computation

```python
# Define inputs
x1 = Value(2.0, label='x1')
w1 = Value(-3.0, label='w1')
b = Value(6.88, label='b')

# Forward pass
n = x1 * w1 + b
o = n.tanh()

# Backward pass
o.backward()

# Gradients are now computed!
print(x1.grad)  # âˆ‚o/âˆ‚x1
print(w1.grad)  # âˆ‚o/âˆ‚w1
print(b.grad)   # âˆ‚o/âˆ‚b
```

---

## ğŸš€ Usage

### Prerequisites

```bash
pip install numpy matplotlib graphviz jupyter
```

### Running the Notebook

1. Clone the repository:
```bash
git clone https://github.com/Timalk16/micrograd-course.git
cd micrograd-course
```

2. Open the Jupyter notebook:
```bash
jupyter notebook micrograd.ipynb
```

3. Run cells sequentially to:
   - Understand numerical differentiation
   - Build the `Value` class step by step
   - Visualize computation graphs
   - Train a simple neural network

### Training a Neural Network

```python
# Create a multi-layer perceptron
mlp = MLP(3, [4, 4, 1])  # 3 inputs, hidden layers of 4, 4, output of 1

# Training data
xs = [[2.0, 3.0, -1.0], [3.0, -1.0, 0.5], [0.5, 1.0, 1.0], [1.0, 1.0, -1.0]]
ys = [1.0, -1.0, -1.0, 1.0]

# Training loop
for epoch in range(20):
    # Forward pass
    ypred = [mlp(x) for x in xs]
    loss = sum([(yout - ygt)**2 for ygt, yout in zip(ys, ypred)])
    
    # Backward pass
    for p in mlp.parameters():
        p.grad = 0.0
    loss.backward()
    
    # Update weights
    for p in mlp.parameters():
        p.data += -0.05 * p.grad
    
    print(f"Epoch {epoch}: Loss = {loss.data:.6f}")
```

---

## ğŸ“ Learning Outcomes

This project demonstrates understanding of:

### Mathematical Foundations
- **Calculus**: Derivatives, chain rule, partial derivatives
- **Numerical Methods**: Finite difference approximation
- **Linear Algebra**: Matrix operations, vector spaces

### Computer Science Concepts
- **Graph Theory**: Topological sorting, DAG traversal
- **Object-Oriented Design**: Class hierarchies, operator overloading
- **Algorithm Design**: Efficient gradient computation

### Deep Learning Principles
- **Backpropagation**: How gradients flow through networks
- **Neural Network Architecture**: Layers, neurons, activations
- **Optimization**: Gradient descent, parameter updates

### Software Engineering
- **Code Organization**: Modular design, reusable components
- **Visualization**: Graph representation and rendering
- **Testing**: Comparison with established frameworks (PyTorch)

---

## ğŸ“Š Project Structure

```
micrograd-course/
â”‚
â”œâ”€â”€ micrograd.ipynb          # Main implementation notebook
â”œâ”€â”€ README.md                # This file
â”‚
â””â”€â”€ Implementation Sections:
    â”œâ”€â”€ Numerical Differentiation
    â”œâ”€â”€ Value Class Definition
    â”œâ”€â”€ Operation Overloading
    â”œâ”€â”€ Backward Propagation
    â”œâ”€â”€ Graph Visualization
    â”œâ”€â”€ Neural Network Components
    â””â”€â”€ Training Example
```

---

## ğŸ”¬ Technical Highlights

### Gradient Computation

The engine correctly computes gradients for:
- **Addition**: `âˆ‚(a+b)/âˆ‚a = 1`, `âˆ‚(a+b)/âˆ‚b = 1`
- **Multiplication**: `âˆ‚(a*b)/âˆ‚a = b`, `âˆ‚(a*b)/âˆ‚b = a`
- **Power**: `âˆ‚(a^n)/âˆ‚a = n*a^(n-1)`
- **Tanh**: `âˆ‚tanh(a)/âˆ‚a = 1 - tanhÂ²(a)`
- **Exponential**: `âˆ‚exp(a)/âˆ‚a = exp(a)`

### Verification

The implementation is verified against PyTorch's autograd engine, producing identical gradient values (within floating-point precision).

---

## ğŸ¯ Educational Value

This project is ideal for:

- **Students** learning deep learning fundamentals
- **Developers** wanting to understand autograd internals
- **Researchers** exploring gradient computation methods
- **Anyone** curious about how neural networks learn

---

## ğŸ“ Notes

- This is an educational implementation focused on clarity over performance
- For production use, consider established frameworks like PyTorch or TensorFlow
- The implementation follows the micrograd approach popularized by Andrej Karpathy

---

## ğŸ¤ Contributing

This is an educational project. Suggestions and improvements are welcome!

---

## ğŸ“„ License

This project is open source and available under the [MIT License](LICENSE).

---

## ğŸ™ Acknowledgments

Inspired by the micrograd implementation and educational content from the deep learning community, particularly the work on understanding neural networks from first principles.

---

<div align="center">

**Built with â¤ï¸ for learning and understanding**

â­ Star this repo if you find it helpful!

</div>

